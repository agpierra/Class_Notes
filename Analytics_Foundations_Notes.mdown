#Day 4

##Chapter 2: Analysis of Variance(ANOVA)


###2.1 Regression Model Diagnostics

Objectives:
* Use the TTest procedure to analyze the differences between two population means
* Verify the assumptions of a two-sample test

**Assumptions**
- Independednt observations
- Normally distributed data for each group
- Equal variances for each group
- (Can use large smaples instead of normally distributed observations due to central limit theorem)



#### Folded F Test for Equality of Variances

```
H_o : sigma1^2 = sigma2^2
H_a : sigma1^2 ~= sigma2^2

F = max(s1^2, s2^2) / min(s1^2, s2^2)
```
The F value is calculated as a ratio of the greater of the two variances divided by the lesser of the two. F tends to be closer to 1.0 if the null hypothesis is true.
Test is valid **only** for independent samples from normal distributions. **Normality is even required for large samples!**

Note: If data is not normally distributed, you can look at plots to determine if equal variance.

![alt text](Pictures/Folded_F_Test.png "Folded F Test for Equality of Variances")

####TTEST Procedure

```
proc ttest data=_SAS-data-set_
	class _variable_;
	var _variables_;
	Paired _variable1*variable2_;
run;
```

Paired - specifies pairs of numeric response variables from which difference scores are calculated (variable1-variable2). A one-sample t test is then performed on the difference scores

####Steps for t-Test for equal/unequal means

1. Check the assumption of equal variances and then use the appropriate test for equal means
2. If calculated F > F' then use the equal variance t-test line in the output to test whether the means of the two populations are equal **(Pooled)**. Otherwise, use the unequal variance t-test **(Scatterthwaite)**.

**Before doing the appropriate test for equal means, you must do a test to see if the variances are equal.** 

####Demo: Two-sample t-test

```
/*st102d01.sas*/
proc ttest data=sasuser.TestScores plots(shownull)=interval;
    class Gender;
    var SATScore;
    title "Two-Sample t-test Comparing Girls to Boys";
run;
```

First you must verify assumptions of t-tests. Look at distribution of each gender to verify the normality of each group (looks fairly normal on page 2-9). 
Q-Q plots also approximate to a normal distritibution, with one outlier - a male scoring 1600 when no other male scored greater than 1400.


Result:
```

                                                        The TTEST Procedure
 
                                                        Variable:  SATScore

                           Gender          N        Mean     Std Dev     Std Err     Minimum     Maximum

                           Female         40      1221.0       157.4     24.8864       910.0      1590.0
                           Male           40      1160.3       130.9     20.7008       890.0      1600.0
                           Diff (1-2)            60.7500       144.8     32.3706                        

                   Gender        Method               Mean       95% CL Mean        Std Dev      95% CL Std Dev

                   Female                           1221.0      1170.7   1271.3       157.4       128.9    202.1
                   Male                             1160.3      1118.4   1202.1       130.9       107.2    168.1
                   Diff (1-2)    Pooled            60.7500     -3.6950    125.2       144.8       125.2    171.7
                   Diff (1-2)    Satterthwaite     60.7500     -3.7286    125.2                                 

                                    Method           Variances        DF    t Value    Pr > |t|

                                    Pooled           Equal            78       1.88      0.0643
                                    Satterthwaite    Unequal      75.497       1.88      0.0644

                                                       Equality of Variances
 
                                         Method      Num DF    Den DF    F Value    Pr > F

                                         Folded F        39        39       1.45    0.2545
```
1. Examine descriptive statistics for each group and their differences
2. Look at the Equality of Variances table. F test has a p-value of 0.2545 > 0.05. Therefore, do NOT reject the null hypothesis of equal variances. We continue as if the variances are equal.
3. Look at the T-Test table for the hypothesis for equal means. Use the equal variance (pooled) t-test.
	* Do not reject null hypothesis that the group means are equal P-value = 0.0643 > 0.05 . 
	* There is no significant difference in average SAT score between boys and girls
	* Note: you can also use confidence intervals. (-3.6950, 125.2) includes 0 (95% confidence interval)


### 2.2 One-Way ANOVA

![alt text](Pictures/Overview_of_statistical_models.png)

**Predictor** = Categorical 
**Response** = Continuous    ->   Use **One-Way ANOVA**

Analysis of variance is a statistical technique used to compare the means of two or more groups of observations or treatments.

*Examples of one-way anova*: 
- Do accountants, on average, earn more than teachers?
- Do people treated with one of two new drugs have higher average T-cell counts than people in the control group?
- Do people spend different amounts of depending on which type of credit card they have?


When there are three or more levels for the grouping variable, you can run a series of t tests between all the pairs of levels. However, a more powerful approach is to analyze all the data simultaneously _(one-way analysis of variance)_. In this case, **the test statistic is the F ratio** rather than the Student's t value.


####Garlic Ranch Example

-Does the type of fertilizer used affect the average weight of garlic grown at the Montana Gourmet Garlic Ranch?

Variables in the data set:
* `Fertilizer`: Type of fertilizer used (1 through 4) where 3 are organic, and one is a chemical fertilizer (as control)
* `BulbWt`: Average garlic bulb weight (in pounds) in the bed
* `Cloves`: The average number of cloves on each bulb
* `BedID`: randomly assigned bed identification number

Printing the data in the sasuse.MGGarlic dataset and create descriptive statistics:

```
/*st102d02.sas*/  /*Part A*/
proc print data=sasuser.MGGarlic (obs=10);
   title 'Partial Listing of Garlic Data';
run;
```

Data shown below:
```
                                      The MEANS Procedure

                                  Analysis Variable : BulbWt

             N
           Obs     N            Mean         Std Dev         Minimum         Maximum
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
            32    32           0.219           0.029           0.152           0.278
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ


                                  Analysis Variable : BulbWt

                     N
     Fertilizer    Obs     N            Mean         Std Dev         Minimum         Maximum
   ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
              1      9     9           0.225           0.025           0.188           0.254

              2      8     8           0.209           0.026           0.159           0.241

              3     11    11           0.230           0.026           0.189           0.278

              4      4     4           0.196           0.041           0.152           0.248
   ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
```

Look at fertilizer groups separately:
```
/*st102d02.sas*/  /*Part B*/
proc means data=sasuser.MGGarlic printalltypes maxdec=3;
    var BulbWt;
    class Fertilizer;
    title 'Descriptive Statistics of Garlic Weight';
run;

/*st102d02.sas*/  /*Part C*/
proc sgplot data=sasuser.MGGarlic;
    vbox BulbWt / category=Fertilizer datalabel=BedID;
    format BedID 5.;
    title "Box and Whisker Plots of Garlic Weight";
run;

```

`PRINTALLTYPES` displays all requested combinations of class variables (all TYPE vaalues) in the printed or displayed output.

`Class *variables*` specifies the variables whose values define the subgroup combinations for the analysis. Do not need to sort the data by class variables.

`Category =` produces separate box plots for each level of the variable listed

In this demo, the design is not balanced. The groups are not equally sized (look at N Obs in the Means procedure).

![alt text](Pictures/Anova_Null_Alternative_Hypothesis.png)


####Partitioning Variability in ANOVA

In ANOVA, Total Variation (measured by the corrected total sum of squares) is partitioned into two components:

1. The Between Group Variation (Model Sum of Squares)
	* Variability explained by the independent variable and therefore represented by the between treatment sum of squares. Calculated as the weighted (by group size) sum of the squared differences between the mean for each group and the overall mean. (SS_M) ![alt text](Pictures/Between_Group_Variation.png)
2. The Within Group variation (Error Sum of Squares)
  * Variability not explained by the model. Also referred to as _within treatement variability or residual sum of squares_. It is calculated as the sum of the squared differences between each observed value and the mean for its group. (SS_E) ![alt text](Pictures/Within_Group_Variation.png)

ANOVA breaks apart the variance of the dependent variable to determine whether the between-group variation is a significant portion of the total variation.

The test statistic (F Ratio) is only a ratio of the model variance to the error variance.

Overall variability in the response variable is calculated as the sum of the squared differences between each observed value and the overall mean. (SS_T) ![alt text](Pictures/Total_Variation.png)

**SS_T = SS_M + SS_E**

The amount of variability in the response that the model explains is given by: (SS_M / SS_T)*100%

The basic measures of variation under the two hypotheses are transformed into a ratio of the model and the error variances that has a known distribution (sample statistic, F ratio) under the null hypothesis that all group means are equal. The F ratio can be used to compute a p-value.

####F Statistic and Critical Values

F = MSM/MSE = SS_M/df_m  / SS_E/df_e

- **Model DF** = Number of treatments minus 1

- **Corrected total DF** = sample size - 1

- **Error DF** = the sample size minus the number of treatments (differenceb etween total DF and model DF)

Mean squares are calculated by taking the sums of squares and dividing by the corresponding degrees of freedom. They can be thought of as variances.
- MSE is an estimate of sigma^2, the constant variance assumed for all treatments
- If mu_i = mu_j for all i ~= j, then the mean square for the model (MSM) is also an estimate of sigma^2
- If mu_i ~= mu_j then MSM estimates sigma^2 plus a positive constant
- The p-value for the test is calculated from the F distribution with appropriate degrees of freedom 
	-(numerator = df_m, denominator = df_e)


R^2 = SS_M / SS_T

- Coefficient of determination is a measure of the proportion of variability explained


####The ANOVA Model

![alt text](Pictures/Fertilizer_Equation.png)
 
 Variables:
 - `Y_ik` = the kth value of the response variable for the ith treatment
 - `mu` = the overall population mean of the response
 - `tau_i` = the difference between the population mean of the ith treatment and the overall mean, mu. This is referred to as the *effect* of treatment i
 - `epsilon_ik` = the difference between the observed value of the kth observation in the ith group and the mean of the ith group (error term)


 #### Proc GLM

 Uses a parameterization of categorical variables in its `Class` statement that will not directly estimate the values of the parameters in the model shown. The correct parameter estimates can be obtained by adding the `solution` option in the `model` statment in `proc glm` and then using simple algebra.
 Parameter estimates and standard errors can also be obtained using the `estimate` statments.

 Fixed effect - researchers are only interested in four specific fertilizers
 Random effect - If the fertilizers used were a sample of many that can be used, the sampling variability of fertilizers would need to be taken into account in the model.

 ##### SAS code

 ```
/*st102d03.sas*/  /*Part A*/
proc glm data=sasuser.MGGarlic;
     class Fertilizer;
     model BulbWt=Fertilizer;
     title 'Testing for Equality of Means with PROC GLM';
run;
quit;


 ```

 * `Class` = specifies classification variables for analysis
 * `Model` = specifies dependent and independent variables for analysis
 * `Means` = computes unadjusted means of the dependent variable for each value of the speicified effect
 * `LSMeans` = produces adjusted means for the outcome variable, broken out by the variable specified and adjusting for any other exlanatory variables included in the `model` statment
 * `output` = specifies an output data set that contains all variables form the input data set and variables that represent statistics fromt he analysis

 ```

                                    Dependent Variable: BulbWt

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                        3      0.00457996      0.00152665       1.96    0.1432

      Error                       28      0.02183054      0.00077966

      Corrected Total             31      0.02641050


                      R-Square     Coeff Var      Root MSE    BulbWt Mean

                      0.173414      12.74520      0.027922       0.219082


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00457996      0.00152665       1.96    0.1432


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00457996      0.00152665       1.96    0.1432
```


Output divided into 3 parts
	- The analysis of variance table
	- Descriptive information
	- Information about the effect of the independent variable in the model


The F statistic and corresponding p-value are reported in the Analysis of Variance table. p-value (0.1432) > 0.05 and therefore, you do not reject the null hypothesis of no difference between the means.

The `BulbWt` Mean is the mean of all the data values in the variable `BulbWt` without regard to `Fertilizer`

For a one-way anova (only one classification variable) the informationa bout the independent variable in the model is an exact duplicate of the model line of the  anova table.

Default plot created is a box plot. *To check validity of Anova model, look at the diagnostic plots*. Code is below to do this:

 ```
/*st102d03.sas*/  /*Part B*/
proc glm data=sasuser.MGGarlic plots(only)=diagnostics;
     class Fertilizer;
     model BulbWt=Fertilizer;
     means Fertilizer / hovtest;
     title 'Testing for Equality of Means with PROC GLM';
run;
quit;

 ```

 Selected means statement option:

 `Hovtest` performs Levene's test for homogeneity (equality) of variances. The null hypothesis is that variances are equal (Levene's is default test)

 `Diagnostics` produces a panel display of diagnostic plots for linear models.

##### Diagnostic plots

Plot in upper left shows residuals vs fitted values from ANOVA model. Any patterns or trends in this plot indicates model misspecification

 Residual histogram and Q-Q plot at bottom left and middle left respectively test for normality assumption
 - Histogram has no unique peak and has short tales, but is approximately symmetric
 - Data values in Q-Q stay close to diagonal reference line. Strong support to assumption of normality distrbuted errors
 - You can look at the following table to check the assumptions of equal variances (output of hovtest option):

```
	             Levene's Test for Homogeneity of BulbWt Variance
                         ANOVA of Squared Deviations from Group Means

                                         Sum of        Mean
               Source            DF     Squares      Square    F Value    Pr > F

               Fertilizer         3    1.716E-6    5.719E-7       0.98    0.4173
               Error             28    0.000016    5.849E-7
```
Null hypothesis is that variances are equal over all the `fertilizer` groups. p-value 0.4173 > 0.05 and therefore you *do not* reject the jull hypothesis.

If variances were not equal, you could add the `welch` option to the `means` statement. 


####SUMMARY

Null Hypothesis: All means are equal
Alternative hypothesis: At least one mean is different

1. Produce descriptive statistics
2. Verify assumptions
	- Independence
	- Errors are normally distributed
	- Error variances are equal for all groups
3. Examine the p-value in the ANOVA table. If the p-value is less than alpha, reject the null hypothesis


### 2.3 ANOVA with Data from a Randomized Block Design

**Objectives**
- Recognize the difference between a completely randomized design aand a randomized block design
- differentiate between observed data and designed experiments
- Use the GLM procedure to analyze data from a randomized block design

**Observational or Retrospective Studies**
- Groups can be naturally occuring (gender ethnicity)
- Random assignment might be unethical or untenable (smoking or credit risk groups)
- Often you look at what already happened instead of following through to the future
- you have little control over other factors contributing to the outcome measure


Bulb weight = Fertilizer + Nuisance Factors + Random Variable

$\gamma_{ijk} = \mu + \alpha_i + \tau_j + \epsilon_{ijk}$

where Nuisance factors could be sun exposure, ph of the soil, rain

Before blocking, the variation due to the nuisance factors is contained within the sum of square errors

Because sector is included in the anova model, any effect caused by the nuisance factors that are common within a sector are accounted for in the Model Sum of Squares and not the Error Sum of Squares. Removing significant effects fromt he Error Sum of squares tends to give more power to the test of the effect of interest. This is because MSE, the denominator of the F statistic, tends to be reduced.

####Including a Blocking Variable in the Model

Additional assumptions are as follows:
- Treatments are randomly assigned within each block.
- The effects of the tratment factor are constant across the levels of the blocking variable

When the effects of the treatment factor are not constant across the levels of the other variable, then this condition is called **interaction**.

####Anova with Blocking in SAS

```
/*st102d04.sas*/
proc glm data=sasuser.MGGarlic_Block plots(only)=diagnostics;
     class Fertilizer Sector;
     model BulbWt=Fertilizer Sector;
     title 'ANOVA for Randomized Block Design';
run;
quit;
```

The blocking variable must be in the model and it must be listed in the class statement.

Output:
```
									Dependent Variable: BulbWt

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                       10      0.02307263      0.00230726       5.86    0.0003

      Error                       21      0.00826745      0.00039369

      Corrected Total             31      0.03134008


                      R-Square     Coeff Var      Root MSE    BulbWt Mean

                      0.736202      9.085064      0.019842       0.218398


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00508630      0.00169543       4.31    0.0162
      Sector                       7      0.01798632      0.00256947       6.53    0.0004


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00508630      0.00169543       4.31    0.0162
      Sector                       7      0.01798632      0.00256947       6.53    0.0004
```


The overall test p-value = 0.0003 indicates that there are significant differences between the means of the garlic bulb weights across fertilizers or blocks (sectors). However, because there is more than one term in the model, you *cannot* tell whether the differences are due to differences among the fertilizers or differences across sectors.

Because our P-value for fertilizer is 0.0162, there is a difference between fertilizers
Because the p-value in our sector is 0.0004, there is a difference between each of the blocks, and therefore blocking was required.

Because we chose 4 specific fertilizers, we can only make inferences on those 4 specific fertilizers (fixed-effects model)

**Diagnostics:**
	- Good random scatter of residuals
	- Decent looking Q-Q test

Now we want to know which pairs of fertilizers are garlic bulb weights different form one another?


###2.4 ANOVA Post Hoc Tests

Objectives
* Perform pairwise comparisons among groups after finding a significant effect of an independent variable in ANOVA
* Demonstrate graphical features in proc GLM for performing post hoc tests
* Interpret a diffogram
* Interpret a control plot

When we control the **comparisonwise error rate** (CER), we fix the level of alpha for a single comparison, without taking into consideration all the pairwise comparisons we are making.

The **experimentwise error rate** (EER) uses an alpha that takes into consideration all the pariwise comparisons that you are making. Chance that you falsely conclude that at least one difference exists is much higher when you consider all possible comparisons.

Control Comparisonwise Error Rate -> Pairwise t-tests

Control Experimentwise Error Rate -> Compair all pairs tukey, compare to control dunnett

All of the multiple comparison methods are requested with options in the `LSMEANS` statment of `proc glm`

**Comparison Control** :  `lsmeans / pdiff=all adjust=t`

**Experimentwise Control**  :  `lsmeans / pdiff=all adjust=tukey` or 
							`pdiff=control('control level') adjust=dunnett`

Tukey's multiple comparison method:
* Appropriate when you consider pairwise comparisons only
* The experimentwise error rate is
	* Equal to alpha when all pairwise comparisons are considered
	* Less than alpha when fewer than all pairwise comparisons are considered

**Use a diffogram to tell whether two group means are statistically significant**

![alt text](Pictures/Diffogram.PNG)


#### Special case of comparing to a control
Comparing to a control is appropriate when there is a natural reference group (ie placebo in a drug trial)

This is an example of the dunnet method

![alt text](Pictures/Control_Plot.png)

If confidence interval is in shaded region, then they are not statistically different.

####Post Hoc Pairwise Comparisons in SAS

```
/*st102d05.sas*/
proc glm data=sasuser.MGGarlic_Block 
         plots(only)=(controlplot diffplot(center));
    class Fertilizer Sector;
    model BulbWt=Fertilizer Sector;
    lsmeans Fertilizer / pdiff=all adjust=tukey;
    lsmeans Fertilizer / pdiff=control('4') adjust=dunnett;
    lsmeans Fertilizer / pdiff=all adjust=t;
    title 'Garlic Data: Multiple Comparisons';
run;
quit;

```
`plot=` **options**:

`controlplot` = requests display in which least squares means are compared against a reference level. LS mean control plots only produced when you specify `pdiff=control` or `adjust=dunnett` in lsmeans statement

`diffplot` = modifies diffogram produced by lsmeans statement with the `pdiff=all` option. `center` marks center point for each comparison.

`lsmeans` **options**:

`pdiff=` = requests p-values for differences, which is the probability of seeing a difference between two means that is as large as the observed means or larger if the two populations means are actually the same.

`adjust=` specifies the adjustment method for multiple comparisons.
* `T` asks that no adjustment made for multiple comparisons
* `Tukey` uses Tukey
* `Dunnet` uses dunnet

Results below show p-values the diffogram represents:

```
                                      Least Squares Means

                                                BulbWt      LSMEAN
                            Fertilizer          LSMEAN      Number

                            1               0.23625000           1
                            2               0.21115125           2
                            3               0.22330125           3
                            4               0.20288875           4


                          Least Squares Means for effect Fertilizer
                             Pr > |t| for H0: LSMean(i)=LSMean(j)

                                  Dependent Variable: BulbWt

                 i/j              1             2             3             4

                    1                      0.0195        0.2059        0.0029
                    2        0.0195                      0.2342        0.4143
                    3        0.2059        0.2342                      0.0523
                    4        0.0029        0.4143        0.0523


NOTE: To ensure overall protection level, only probabilities associated with pre-planned
      comparisons should be used.
```

By Tukey test, 1 and 4 are different.
By Dunnett, 1 and 4 are different
By T test, 1 and 4 are different and 2 and 1 are different (NOT TRUE DONT USE THIS TEST)


### 2.5 Two-Way ANOVA with Interactions

Objectives
* Fit a two-way ANOVA model
* Detect interactions between factors
* Analyze the treatments when there is a significant interaction

N-way ANOVA

![alt text](Pictures/Nway_Anova.png)

Drug example: The purpose of the study is to look at the effect of a new prescription drug on blood pressure

`DrugDose` dosage level of drug (1, 2, 3, 4) corresponding to (Placebo, 50mg, 100mg, 200mg)

`Disease` heart disease category

`BloodP` change in diastolic blood pressure after 2 weeks treatment

![alt text](Pictures/BloodP_Model.png)

An interaction occurs when the differences between group means on one variable change at different levels of another variable.

If an interaction exists between any factors, the tests for the individual factor effects might be misleading, deo to masking of the effects by the interaction. This is especially true for unbalanced data.

If the lines of an interaction graph are parallel, we can say that there is no interaction between variables.

Analyze the main effects with the interaction in the model:

$\Gamma_{ijk} = \mu + \alpha_i + \Beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$

or delete the interaction from the model and then analyze the main effects:

$\Gamma_{ijk} = \mu + \alpha_i + \Beta_j + \epsilon_{ijk}$

#### Two-Way ANOVA with Interactions in SAS

```
/*st102d06.sas*/  /*Part A*/
proc print data=sasuser.drug(obs=10);
    title 'Partial Listing of Drug Data Set';
run;

/*st102d06.sas*/  /*Part B*/
proc format;
    value dosefmt 1='Placebo'
                  2='50 mg'
                  3='100 mg'
                  4='200 mg';
run;

proc means data=sasuser.drug
           mean var std nway;
    class Disease DrugDose;
    var BloodP;
  format DrugDose dosefmt.;
    output out=means mean=BloodP_Mean;
    title 'Selected Descriptive Statistics for Drug Data Set';
run;
```

`nway` = when you include `class` variables, `nway` specifies that the output data set contains only statistics for the observations with the hightest type and way values.

Results:

```

                                  Analysis Variable : BloodP

                      Drug         N
           Disease    Dose       Obs            Mean        Variance         Std Dev
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
           A          Placebo     12       1.3333333     183.1515152      13.5333483

                      50 mg       16      -9.6875000     356.7625000      18.8881577

                      100 mg      13     -26.2307692     329.0256410      18.1390640

                      200 mg      18     -22.5555556     445.0849673      21.0970369

           B          Placebo     15      -8.1333333     285.9809524      16.9109714

                      50 mg       15       5.4000000     479.1142857      21.8886794

                      100 mg      14      24.7857143     563.7197802      23.7427838

                      200 mg      13      23.2307692     556.3589744      23.5872630

           C          Placebo     14       0.4285714     411.8021978      20.2929100

                      50 mg       13      -4.8461538     577.6410256      24.0341637

                      100 mg      14      -5.1428571     195.5164835      13.9827209

                      200 mg      13       1.3076923     828.5641026      28.7847894
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ

```



Let's plot it!

```
/*st102d06.sas*/  /*Part C*/
proc sgplot data=means;
    series x=DrugDose y=BloodP_Mean / group=Disease markers;
    xaxis integer;
    title 'Plot of Stratified Means in Drug Data Set';
  format DrugDose dosefmt.;
run;
```

`series` statement creates a line plot

`markers` adds data point markers to the series plot data points

`xaxis integer` forces the x-axis to have tick marks only at integer values


```
/*st102d06.sas*/  /*Part D*/
proc glm data=sasuser.drug order=internal;
    class DrugDose Disease;
    model Bloodp=DrugDose Disease DrugDose*Disease;
    title 'Analyze the Effects of DrugDose and Disease';
    title2 'Including Interaction';
  format DrugDose dosefmt.;
run;
quit;
```

results:

```

Dependent Variable: BloodP

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                       11      36476.8353       3316.0759       7.66    <.0001

      Error                      158      68366.4589        432.6991

      Corrected Total            169     104843.2941


                      R-Square     Coeff Var      Root MSE    BloodP Mean

                      0.347918     -906.7286      20.80142      -2.294118


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      DrugDose                     3        54.03137        18.01046       0.04    0.9886
      Disease                      2     19276.48690      9638.24345      22.27    <.0001
      DrugDose*Disease             6     17146.31698      2857.71950       6.60    <.0001


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      DrugDose                     3       335.73526       111.91175       0.26    0.8551
      Disease                      2     18742.62386      9371.31193      21.66    <.0001
      DrugDose*Disease             6     17146.31698      2857.71950       6.60    <.0001

```

Although the P-value for drug dose is high (almost 1), it does not mean that there is no effect in drug dose. If there is an interaction between drug dose and disease, we have to keep our main effect.

Looking on the graph, you can see that the drug dose relationship for disease A and disease B 'cancel each other out'



```
/*st102d06.sas*/  /*Part E*/
ods graphics off;
ods select LSMeans SlicedANOVA;
proc glm data=sasuser.drug order=internal;
    class DrugDose Disease;
    model Bloodp=DrugDose Disease DrugDose*Disease;
    lsmeans DrugDose*Disease / slice=Disease;
    title 'Analyze the Effects of DrugDose';
    title2 'at Each Level of Disease';
  format DrugDose dosefmt.;
run;
quit;

ods graphics on;

```

Results for lsmeans:

```
 DrugDose*Disease Effect Sliced by Disease for BloodP

                                       Sum of
            Disease        DF         Squares     Mean Square    F Value    Pr > F

            A               3     6320.126747     2106.708916       4.87    0.0029
            B               3           10561     3520.222833       8.14    <.0001
            C               3      468.099308      156.033103       0.36    0.7815

```

Taking disease x, puts it in its own data set, and run an anova against drug dose




# Day 5

## Chapter 5: Categorical Data Analysis

### 5.1 Describing Categorical Data

Objectives
* Examine the distribution of categorical variables
* Do preliminary examinations of associans between variables

Categorical variables association
* An associacion exists between two categorical variables if the distribution of one variable changes when the other variable changes
* For no assiciation, the distribtion of the first variable stays constant for different levels of the other variable

#### Crosstabulation Tables

Crosstabulation table shows the number of observations for each combination of the row and column variables.

By default, 4 measures in each cell
- Frequency
- Percent
- Row Percent
- Column Percent

#### Proc Freq

```
proc freq data=sas-data-set;
  tables table-requests / options;
run;
```

`tables` requests tables and specifies options for producing tests.
- General form is variable1*variable2*... where any number can be put in the tables statment.
  - For 2 way, first represents rows and second represents columns

#### Titanic example

- 2,223 passengers
- 1,517 fatalities

Variables:
- Survival statuss (1 or 0)
- Age
- Gender
- Class (1,2,3)
- Fare (cumulative total for a purchase for each person in a party)

Code:

```
/*st105d01.sas*/
title;
proc format;
    value survfmt 1 = "Survived"
                  0 = "Died"
                  ;
run;

proc freq data=sasuser.Titanic;
    tables Survived Gender Class
           Gender*Survived Class*Survived /
           plots(only)=freqplot(scale=percent);
    format Survived survfmt.;
run;

proc univariate data=sasuser.Titanic noprint;
    class Survived;
    var Age ;
    histogram Age;
    inset mean std median min max / format=5.2 position=ne;
    format Survived survfmt.;
run;
```

`FREQPLOT(<suboptions>)` requests a frequency plot. Available for frequency and crosstabulation tables

`(Scale=)` specifies the scale of the frequencies to display. Default is `Scale=freq`. Can also use `scale=percent`

In proc univariate, the line that reads:
`inset mean std median min max / format=5.2 position=ne;`
will put all of those variables in the same plot of the histogram.

We look at the distribution of survived, gender and class. Also look at the crosstabulation of gender*survived, and class*survived.

```

                                      The FREQ Procedure

                                                      Cumulative    Cumulative
                 Survived    Frequency     Percent     Frequency      Percent
                 ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                 Died             809       61.80           809        61.80
                 Survived         500       38.20          1309       100.00


                                                     Cumulative    Cumulative
                  Gender    Frequency     Percent     Frequency      Percent
                  ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                  female         466       35.60           466        35.60
                  male           843       64.40          1309       100.00


                                                    Cumulative    Cumulative
                  Class    Frequency     Percent     Frequency      Percent
                  ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                      1         323       24.68           323        24.68
                      2         277       21.16           600        45.84
                      3         709       54.16          1309       100.00


                                  Table of Gender by Survived

                              Gender     Survived

                              Frequency‚
                              Percent  ‚
                              Row Pct  ‚
                              Col Pct  ‚Died    ‚Survived‚  Total
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              female   ‚    127 ‚    339 ‚    466
                                       ‚   9.70 ‚  25.90 ‚  35.60
                                       ‚  27.25 ‚  72.75 ‚
                                       ‚  15.70 ‚  67.80 ‚
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              male     ‚    682 ‚    161 ‚    843
                                       ‚  52.10 ‚  12.30 ‚  64.40
                                       ‚  80.90 ‚  19.10 ‚
                                       ‚  84.30 ‚  32.20 ‚
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              Total         809      500     1309
                                          61.80    38.20   100.00


```

More women survived while more men died according to our crosstabulation table. It also turns out that as the higher class you are, the more chance you have of surviving (most of the lower class were men).

### 5.2 Tests of Association

Objectives
- Perform a chi-square test for association
- Examine the strength of association
- Calculate exact p-values
- Perform a Mantel-Haenszel chi-square test

Using contingency table analysis!

Assumng the same titanic example:

**Null Hypothesis** = There is no association between gender and surival. The probability of surviving the titanic class was the same whether you were male or female

**Alternative Hypothesis** = There **is** an association between gender and survival.


#### Chi-Square Test

**No Association** - Observed frequencies = Expected frequencies

**Association** - Observed frequencies ~= Expected frequencies

![alt text](Pictures/chi_squared_eq.png)

This equation tests whether an association exists. It **does not** measure the strength of an association. It depends on sample size.

The p-value for the chi-square test only indicates how confident you can be that the null hypothesis of no association is false. Doubling the size of the sample by duplicating each observation will double the value of the chi-square statistic, even though the strength of the association does not change.

**Cramer's V statistic** can measure the strength of the association.

- Has a range of -1 to 1 for 2 by 2 tables and 0 to 1 for larger tables. Values farther from 0 indicate stronger association

#### Odds Ratio

Odds ratio indicates how much more likely, **with respect to odds**, a certain event occurs in one group relative to its occurence in another group

![alt test](Pictures/odds_eq.png)

**Odds** are calculated **from** probabilities.

**Odds ratio** is the ratio of odds, as seen below:

![alt test](Pictures/odds_ratio_ex.png)

Group A had 1/3 the odds of group B **OR** Group B had 3 time the odds of having the outcome compared to group A.

#### Chi-Square Test in SAS

```
/*st105d02.sas*/
ods graphics off;
proc freq data=sasuser.Titanic;
    tables (Gender Class)*Survived
          / chisq expected cellchi2 nocol nopercent 
            relrisk;
    format Survived survfmt.;
    title1 'Associations with Survival';
run;

ods graphics on;
```

`Chisq` produces chi-square test of association and the measures of association based on chi-square statistic

`Expected` prints the expected cell frequencies under the hypothesis of no association

`CellChi2` prints each cell's contribution to th total chi-square statistic

`Nocol` suppresses column percentages

`Nopercent`suppresses cell percentages

`Relrisk` prints a table with risk ratios (probability ratios) and odds ratios

Results:

```
                       The FREQ Procedure

                                  Table of Gender by Survived

                           Gender          Survived

                           Frequency      ‚
                           Expected       ‚
                           Cell Chi-Square‚
                           Row Pct        ‚Died    ‚Survived‚  Total
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           female         ‚    127 ‚    339 ‚    466
                                          ‚    288 ‚    178 ‚
                                          ‚ 90.005 ‚ 145.63 ‚
                                          ‚  27.25 ‚  72.75 ‚
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           male           ‚    682 ‚    161 ‚    843
                                          ‚    521 ‚    322 ‚
                                          ‚ 49.753 ‚ 80.501 ‚
                                          ‚  80.90 ‚  19.10 ‚
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           Total               809      500     1309


                          Statistics for Table of Gender by Survived

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     1    365.8869    <.0001
                    Likelihood Ratio Chi-Square    1    372.9213    <.0001
                    Continuity Adj. Chi-Square     1    363.6179    <.0001
                    Mantel-Haenszel Chi-Square     1    365.6074    <.0001
                    Phi Coefficient                      -0.5287
                    Contingency Coefficient               0.4674
                    Cramer's V                           -0.5287


                                     Fisher's Exact Test
                              ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                              Cell (1,1) Frequency (F)       127
                              Left-sided Pr <= F       7.351E-83
                              Right-sided Pr >= F         1.0000

                              Table Probability (P)    6.705E-83
                              Two-sided Pr <= P        7.918E-83
```

The cell for survived=1 and Gender=female has the highest Chi-Square value (145.63). It contributes the most to the chi-square statistic.

p-value for chi-square statistic is <.0001 < 0.05. We reject null hypothesis. **There is evidence of an association between gender and survived**.

Strength of this association is realatively strong since Cramer's V is -0.5287

```
 The FREQ Procedure

                          Statistics for Table of Gender by Survived

                          Estimates of the Relative Risk (Row1/Row2)

               Type of Study                   Value       95% Confidence Limits
               ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
               Case-Control (Odds Ratio)      0.0884        0.0677        0.1155
               Cohort (Col1 Risk)             0.3369        0.2894        0.3921
               Cohort (Col2 Risk)             3.8090        3.2797        4.4239

                                      Sample Size = 1309

```

**Relative Risk** table shows another measure of strength association.
- Odds ratio shown in first row of table with 95% confidence limits. Interpreted as the odds of a top row value to be in the left column compared with the same odds in the bottom row.
- In this case, a female has 0.0884 (9%) of the odds of dying compared with a male.

**Cohort estimates** for each column interpreted as **probability ratios, not odds ratios**. You get the choice of assessing probabilities of left column (col1) or right column (col2).
  - The col1 risk shows the ratio of probabilities of females to males being in the left column (27.25/80.90)=0.3369

##### When not to use Chi-Squared

Do not use assymptotic chi-squared test when more than 20% of cells have expected counts less than 5!

p-values are based on the assumption that the test statistic follows a particular distribution when the sample size is sufficiently large.

For small sample size, we calculate **Exact** p-values

For large sample size, we calculate **Asymptotic** p-values. It takes a lot of time and memory to compute exact p-values since SAS calculates *every* possible combination of crosstabulation frequencies.

Exact p-values for Pearson Chi-Square

![alt text](Pictures/exact_pvalues_pearsonchi.png)

The exact p-value is the sum of probabilities of all tables with X^2 values **as great or greater** than that of the observed table!

#### Exact p-values for Pearson Chi-Square in SAS

```
/*st105d03.sas*/
ods graphics off;
proc freq data=sasuser.exact;
   tables A*B / chisq expected cellchi2 nocol nopercent;
   title "Exact P-Values";
run;

ods graphics on;

```

Results:

```
                               Statistics for Table of A by B

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     1      2.1000    0.1473
                    Likelihood Ratio Chi-Square    1      2.8306    0.0925
                    Continuity Adj. Chi-Square     1      0.3646    0.5460
                    Mantel-Haenszel Chi-Square     1      1.8000    0.1797
                    Phi Coefficient                      -0.5477
                    Contingency Coefficient               0.4804
                    Cramer's V                           -0.5477

                     WARNING: 100% of the cells have expected counts less
                              than 5. Chi-Square may not be a valid test.


                                     Fisher's Exact Test
                              ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                              Cell (1,1) Frequency (F)         0
                              Left-sided Pr <= F          0.2857
                              Right-sided Pr >= F         1.0000

                              Table Probability (P)       0.2857
                              Two-sided Pr <= P           0.4286

                                        Sample Size = 7

```

Warning suggests that p-value should not be trusted.

Want to report the two-sided Pr <= P value.

Difference between exact p-value (0.4286) and asymptotic p-value (0.1473). Exact tests tend to be more conservative than asymptotic tests


#### Ordinal Variables - Mantel-Haenszel Chi-square test

**Null Hypothesis** : There is no ordinal association between the row and column variables

**Alternative Hypothesis** : There is an ordinal association between the row and column variables

The Mantel-Haenszel Chi-Square Test determines whether an ordinal association exists. **Does Not** measure the strength of the ordinal association
- More powerful than the general association chi-squared statistic for detecting an ordinal association
  - All of the Mantel-Haenszel statistic's power is concentrated toward that objective
  - Power of the general association statistic is dispersed over a greater number of alternatives

#### Spearman Correlation Statistic

To measure strength of ordinal association, you can use Spearman
- Has range between -1 and 1
- Has values close to 1 if there is relatively high degree of positive correlation
- Has values close to -1 if there is a relatively high degree of negative correlation
- Appropriate only if both variables are ordinal scaled and the values are in logical order

**Spearman** correlation uses ranks of the data

**Pearson** correlation uses the observed values when variable is numeric

#### Using SAS

```
/*st105d04.sas*/
ods graphics off;
proc freq data=sasuser.Titanic;
    tables Class*Survived / chisq measures cl;
    format Survived survfmt.;
    title1 'Ordinal Association between CLASS and SURVIVAL?';
run;

ods graphics on;
```

`chisq` produces Pearson chi-square, likelihood-ratio chi-square, and the Mantel-Haenszel chi-square. Also produces measures of association based on chi-square such as the phi coefficient, contingency coefficient and Cramer's V

`measures` produces Spearman correlation statistic and other measures of association

`CL` produces confidence bounds for the `measures` statistics

Results of mantel-Haenszel:

```
                         Statistics for Table of Class by Survived

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     2    127.8592    <.0001
                    Likelihood Ratio Chi-Square    2    127.7655    <.0001
                    Mantel-Haenszel Chi-Square     1    127.7093    <.0001
                    Phi Coefficient                       0.3125
                    Contingency Coefficient               0.2983
                    Cramer's V                            0.3125

```

Results of spearman:

```

                           Statistics for Table of Class by Survived

                                                                          95%
         Statistic                              Value       ASE     Confidence Limits
         ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
         Gamma                                -0.5067    0.0375    -0.5801    -0.4332
         Kendall's Tau-b                      -0.2948    0.0253    -0.3444    -0.2451
         Stuart's Tau-c                       -0.3141    0.0274    -0.3677    -0.2604

         Somers' D C|R                        -0.2613    0.0226    -0.3056    -0.2170
         Somers' D R|C                        -0.3326    0.0286    -0.3887    -0.2765

         Pearson Correlation                  -0.3125    0.0267    -0.3647    -0.2602
         Spearman Correlation                 -0.3097    0.0266    -0.3619    -0.2576

         Lambda Asymmetric C|R                 0.1540    0.0331     0.0892     0.2188
         Lambda Asymmetric R|C                 0.0317    0.0320     0.0000     0.0944
         Lambda Symmetric                      0.0873    0.0292     0.0301     0.1445

         Uncertainty Coefficient C|R           0.0734    0.0127     0.0485     0.0983
         Uncertainty Coefficient R|C           0.0485    0.0084     0.0320     0.0650
         Uncertainty Coefficient Symmetric     0.0584    0.0101     0.0386     0.0782

                                      Sample Size = 1309

```

Spearman correlation (-0.3097) indicates that there is amoderate, negative ordinal relationship between class and survival

ASE is asymptotic standard error (0.0266). Measure of standard error for larger samples (larger than 25 foe each degree of freedom in the Pearson chi-square stat)

95% confidence interval (-0.3619, -0.2576) for spearman correlation statistic does not conatain 0, and tehrefore the relationship is significant at the 0.05 significance level.


### Introduction to Logistic Regression

Objectives
- Define the concepts of logistic regression
- Fit a binary logistic regression model using the `logistic` procedure
- Describe the standard output form the logistic procedure with one continuous predictor variable
- read and interpret odds ratio talbes and plots

![alt text](Pictures/cont_vs_cat.png)

If response variable is dichotomous (two categories), appropriate logistic regression model is binary logistic regression

If more than 2 categories within response variable, then there are 2 possibilities
- If response is nominal -> fit a *nominal* logistic regression model
- If response is ordinal -> fit a *ordinal* logistic regression model

![alt text](Pictures/ols_reg.png)     vs    ![alt text](Pictures/lpm_model.png)

But can we really use a linear mondel?

Also, there is no such thing as an observed probability -> least squares methods cannot be used since there are no residuals.

Logistic regression model:

![alt text](Pictures/log_reg_model.png)

Logit Transformation:

![alt text](Pictures/logit_t.png)

`i` = indexes all cases (observations)

`p_i` = probability that the even (ex. a sale) occurs in the ith case

`ln` = natural log 

- Modeling probability has bounds of 0 and 1
- The logit has no upper or lower bounds
- Logit is the natural log of the odds

##### Assumption in logistic regression

Logit has a linear relationship with the predictor variables.

We are assuming that the relationship between our variable and probability follows the 'S' curve. This is so that we get a linear relationship between the logit and our variable.

Our logistic regression model therefore becomes:

![alt text](Pictures/real_log_reg_model.PNG)

#### Proc logistic

Code:

```
proc logistic data=sas-data-set options
  class variables / options;
  model response=predictors / options;
  units independent1=list ... / options;
  oddsratio 'label' variable / options;
  output out=sas-data-set keyword=name / options;
run;
```

`class` names the classification variables to be used in the analysis. Must precede `model` statement. By default, will be analyzed using effects coding

`model` specifies response and predictor variables

`output` creates output data set from input data and any requested statistics

`units` enables you to obtain odds ratio estimate for specified change in a predictor variable

`oddsratio` produces odds ratios for variables even when variables are involved in interactions with other covariates, and for classification variables that use any parameterization.

 ```
/*st105d05.sas*/
proc logistic data=sasuser.Titanic
              plots(only)=(effect oddsratio);
    model Survived(event='1')=Age / clodds=pl;
    title1 'LOGISTIC MODEL (1):Survived=Age';
run;
```

`effect` requests a plot of predicted prob on y axis by predictor on the x axis

`oddsratio` requests a plot of the odds ratio (along with its (1-alpha) confidence limits). Width can be changed using `alpha=` option in `proc logistic` statement

`(event=)` specifies the event category for the binary response model. `proc logistic` models the probability of each event. Can specify value of event category in quotes or specify one of the following keywords:
- `first` designates the first ordered category as the evnet
- `last` designates the last ordered category as the event

`clodds=pl` requests **profile likelihood** confidence intervals for the odds ratio of all predictor variables, which are desirable for small sample sizes.

```
 Response Profile

                               Ordered                      Total
                                 Value     Survived     Frequency

                                     1            0           619
                                     2            1           427

                              Probability modeled is Survived=1.

NOTE: 263 observations were deleted due to missing values for the response or explanatory
      variables.


                                   Model Convergence Status

                        Convergence criterion (GCONV=1E-8) satisfied.


                                     Model Fit Statistics

                                                         Intercept
                                          Intercept            and
                            Criterion          Only     Covariates

                            AIC            1416.620       1415.301
                            SC             1421.573       1425.207
                            -2 Log L       1414.620       1411.301

```

The smaller the SC the better. Smaller the AIC the better.

```

                                    The LOGISTIC Procedure

                            Testing Global Null Hypothesis: BETA=0

                    Test                 Chi-Square       DF     Pr > ChiSq

                    Likelihood Ratio         3.3191        1         0.0685
                    Score                    3.3041        1         0.0691
                    Wald                     3.2932        1         0.0696


                           Analysis of Maximum Likelihood Estimates

                                             Standard          Wald
              Parameter    DF    Estimate       Error    Chi-Square    Pr > ChiSq

              Intercept     1     -0.1335      0.1448        0.8501        0.3565
              Age           1    -0.00800     0.00441        3.2932        0.0696


                 Association of Predicted Probabilities and Observed Responses

                      Percent Concordant      51.3    Somers' D    0.050
                      Percent Discordant      46.4    Gamma        0.051
                      Percent Tied             2.3    Tau-a        0.024
                      Pairs                 264313    c            0.525


               Odds Ratio Estimates and Profile-Likelihood Confidence Intervals

                  Effect         Unit     Estimate     95% Confidence Limits

                  Age          1.0000        0.992        0.983        1.001

```

For global F-test, we can use either Likelihood Ratio, Score or Walk. **Use Likelihood Ratio**.

Our model is not very good to begin with (Likelihood p-value > 0.05)

For specific beta value, we use Analysis of Maximum Likelihood Estimates table. Age is not statistically significant (P-value = 0.0696 >0.05)

For odds ratio estimate, as age increases by 1, you have 0.992 times the odds of surviving (estimate).

Aka: As age increase by one, the odds of surviving decrease by 0.008%
- odds after / odds before = 0.250

![alt text](Pictures/lrm_odds_ratio.png)

You can also look at the confidence interval in the odds to see whether age is a statistically significant variable.

![alt text](Pictures/lrm_odds_ratio_ci.png)

Because we are 95% confident that the true odds ratio is between 0.983 and 1.001, and this interval includes 1.000, the odds ratio is not significant at the 0.05 level



##### Model assessment: Comparing pairs

Counting concordant, discordant, and tied pairs is a way to assess how well the model predicts its own data and thus, how well the model fits

In general, want high percentage of concordant pairs, and low percentages of discordant and tied pairs

We must compare everyone who had the outcome of interest against everyone who did not

Concordant pair
- Dead at age 30 P(survived) = 0.4077
- Survived at age 20 P(survived) = 0.4274

Discordant pair
- Dead at age 35 P(S) = 0.3981
- Survived at age 45 P(S) = 0.3791

Tied pair
- 2 50 year olds, one dead one alive P(S) = 0.3697

Association of Predicted Probabilities and Observed Responses table lists the several measures of association to help assess predictive ability of logistic model.


### 5.4 Logistic Regression with Categorical Predictors

Objectives
- State how a logistic model with categorical predictors does and does not differe from one with continuous predictors
- Describe what a `class` statement does
- Define the standard output from the `logistic` procedure with categorical predictor variables

![alt text](Pictures/cat_vs_cat_log.png)

#### Class statement

Creates a set of 'design variables' representing the information in the categorical variables
- Character variables cannot be used, as is, in a model
- Design variables are ones actually used in model calculations
- Exist several 'parameterizations' available in `proc logistic`

**Note:** By default, SAS uses effects coding

![alt text](Pictures/effects_coding.png)

Number of design variables created is number of levels of the `class` variable minus 1

Effects coding measures the difference between a design variable and the overall average.

##### Effects Coding example

![alt text](Pictures/logit_effect_coding.png)

If B_x is close to zero, then there is no difference (probabilistically) between D_x and the average

##### Reference (Dummy) Coding

![alt text](Pictures/dummy_coding.png)

Reference coding measures the difference between a design variable and a reference variable.

Each non reference B_x represents the difference of the logits for level x and the reference level

The reference B_x represents the value of the logit when at the reference level

Reference coding and effects coding does not necessarily agree in p-values because they test for different things.

#### Multiple Logistic Regression

```
/*st105d06.sas*/
proc logistic data=sasuser.Titanic plots(only)=(effect oddsratio);
    class Gender(ref='male') Class(ref='3') / param=ref;
    model Survived(event='1')=Age Gender Class / clodds=pl;
    units age=10;
    title1 'LOGISTIC MODEL (2):Survived=Age Gender Class';
run;
```

`units` enables you to specify units of change for continuous explanatory variables so that customized odds ratios can be estimated

`(ref='level') specifies the event category chosen as the reference level when using reference or effect parameterization. Default is `ref=last`

`param=` specifies parametrization. Can specify for each variable name by typing it within parentheses after variable name.

Results:

```
                                   Class Level Information

                                                        Design
                                Class      Value      Variables

                                Gender     female      1
                                           male        0

                                Class      1           1      0
                                           2           0      1
                                           3           0      0


                                   Model Convergence Status

                        Convergence criterion (GCONV=1E-8) satisfied.


```

SAS will show coding itself if you do class statement

```
                                    The LOGISTIC Procedure

                                     Model Fit Statistics

                                                         Intercept
                                          Intercept            and
                            Criterion          Only     Covariates

                            AIC            1416.620        992.315
                            SC             1421.573       1017.079
                            -2 Log L       1414.620        982.315


                            Testing Global Null Hypothesis: BETA=0

                    Test                 Chi-Square       DF     Pr > ChiSq

                    Likelihood Ratio       432.3052        4         <.0001
                    Score                  386.1522        4         <.0001
                    Wald                   277.3202        4         <.0001


                                  Type 3 Analysis of Effects

                                                  Wald
                          Effect      DF    Chi-Square    Pr > ChiSq

                          Age          1       29.6314        <.0001
                          Gender       1      226.2235        <.0001
                          Class        2      103.3575        <.0001


                           Analysis of Maximum Likelihood Estimates

                                                Standard          Wald
          Parameter           DF    Estimate       Error    Chi-Square    Pr > ChiSq

          Intercept            1     -1.2628      0.2030       38.7108        <.0001
          Age                  1     -0.0345     0.00633       29.6314        <.0001
          Gender    female     1      2.4976      0.1661      226.2235        <.0001
          Class     1          1      2.2907      0.2258      102.8824        <.0001
          Class     2          1      1.0093      0.1984       25.8849        <.0001


```

AIC, SC and -2 Log L have all dropped, which is good.

Likelihood ratio is good, and therefore one of are variables is statistically significant.

Type 3 Analysis of effects show that age, gender and class are all statistically significant to predict survival. Significance in this table means that some difference in level of a variable is significant.

Analysis of Maximum likelihood estimates shows that both class 1 and class 2 are statistically different than class 3 to model who survived.

```
                            The LOGISTIC Procedure

                 Association of Predicted Probabilities and Observed Responses

                      Percent Concordant      83.8    Somers' D    0.680
                      Percent Discordant      15.8    Gamma        0.683
                      Percent Tied             0.4    Tau-a        0.329
                      Pairs                 264313    c            0.840


               Odds Ratio Estimates and Profile-Likelihood Confidence Intervals

           Effect                        Unit     Estimate     95% Confidence Limits

           Age                        10.0000        0.708        0.625        0.801
           Gender female vs male       1.0000       12.153        8.823       16.925
           Class  1 vs 3               1.0000        9.882        6.395       15.513
           Class  2 vs 3               1.0000        2.744        1.863        4.059

```

We want high values of concordant, and high values of discordant. This model is much better than the old model.

9.882x the odds of survival if you were a class 1 traveller compared to class 3

No odds ratios confidence intervals contain 1, therefore all variables are all significant.


### 5.5 Stepwise Selection with Interaction

Objectives
- Fit a multiple logistic regression model with main effects and interactions using the backward elimination method
- Explain interactions using graphs

![alt text](Pictures/contcat_vs_cat.png)

Similar to proc reg, you can find a best subset model with `proc logistic` stepwise methods. **Default selection criteria changes to 0.05 for forward, backward, stepwise for slentry and slstay**.

#### Model Hierarchy

Model hierarchy refers to the requirement that, for any term to be in the model, all effects contained in the term must be present in the model.

For a more customized analysis, the `Hierarchy=` option specifies whether the hierarchy is maintained and whether a single effect or multiple effects are allowed to enter or leave the model in one step for forward, backward, and stepwise selection.

##### SAS code

```
/*st105d07.sas*/  /*Part A*/
proc logistic data=sasuser.Titanic plots(only)=(effect oddsratio);
    class Gender(ref='male') Class(ref='3') / param=ref;
    model Survived(event='1')=Age|Gender|Class @2 / 
          selection=backward clodds=pl slstay=0.01;
    units age=10;
    title1 'LOGISTIC MODEL (3): Backward Elimination '
           'Survived=Age|Gender|Class';
run;
```

The bar notation with the @2 constructs a model with all the main effects and the two-factor interactions. @3 constructs a model with all of the main effects, two way and three-factor interactions. 

`selection=` specifies method to select variables in model. Can be: `Backward`, `forward`, `selction`, `none`. `score represents best subset selection (default is `none`)

```

                                    The LOGISTIC Procedure

                                   Residual Chi-Square Test

                              Chi-Square       DF     Pr > ChiSq

                                 13.2931        3         0.0040


NOTE: No (additional) effects met the 0.01 significance level for removal from the model.


                                Summary of Backward Elimination

                      Effect                       Number          Wald
              Step    Removed              DF          In    Chi-Square    Pr > ChiSq

                 1    Age*Gender            1           5        4.3264        0.0375
                 2    Age*Class             2           4        8.8477        0.0120


                                  Type 3 Analysis of Effects

                                                     Wald
                       Effect            DF    Chi-Square    Pr > ChiSq

                       Age                1       32.5663        <.0001
                       Gender             1       40.0553        <.0001
                       Class              2       44.4898        <.0001
                       Gender*Class       2       43.9289        <.0001


                           Analysis of Maximum Likelihood Estimates

                                                   Standard          Wald
        Parameter                DF    Estimate       Error    Chi-Square    Pr > ChiSq

        Intercept                 1     -0.6552      0.2113        9.6165        0.0019
        Age                       1     -0.0385     0.00674       32.5663        <.0001
        Gender       female       1      1.3970      0.2207       40.0553        <.0001
        Class        1            1      1.5770      0.2525       38.9980        <.0001
        Class        2            1     -0.0242      0.2720        0.0079        0.9292
        Gender*Class female 1     1      2.4894      0.5403       21.2279        <.0001
        Gender*Class female 2     1      2.5599      0.4562       31.4930        <.0001

```

Selection is done for the best model, then gives you the results of that model.

Age and gender are useful.

Class 1 is different than class 3. Class 2 is not statistically different than class 3.

However, looking at gender and class, we see that females of class 2 are different than females of class 3.

Class bothers the relationship between gender and survival.

```
/*st105d07.sas*/  /*Part B*/
proc logistic data=sasuser.Titanic 
              plots(only)=oddsratio(range=clip);
    class Gender(ref='male') Class(ref='3') / param=ref;
    model Survived(event='1')=Age Gender|Class;
    units age=10;
    oddsratio Gender / at (Class=ALL) cl=pl;
    oddsratio Class / at (Gender=ALL) cl=pl;
    oddsratio Age / cl=pl;
    title1 'LOGISTIC MODEL (3.1): Survived=Age Gender|Class';
run;
```

`range=` with suboptions (min, max)|CLIP, specifies the range of the range of the displayed odds ratio axis. `Range=clip` has same effect as specifying the minimum odds ratio as min and the max odds ratio as max.

`oddsratio` produces odds ratios for a variable even when the variable is involved in interactions with other covariates, and for classification variables that use any parameterization. 

`AT` specifies fixed levels of the interacting covariates. 


# Summary of everything

In morning, take the concept assessment.
- No hand calculation
- No computer calculation
- No calculations
- Formula sheet provided
- Bring writing utensil (calculator if you want)
- True/false, multiple choice, short answer
  - LSR line
  - What is variance
  - All concept questions
- 3 hours to take it

In afternoon, SAS exam
- Open everything
- 3 problems already covered
  - Exercises/code done in class
  - Walking through all the problems
    - Explore data set
    - Give me blah blah blah
  - Need to provide output
  - Need to interpret output
  - Need to hand in code