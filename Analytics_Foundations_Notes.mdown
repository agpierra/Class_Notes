#Day 4

##Chapter 2: Analysis of Variance(ANOVA)


###2.1 Regression Model Diagnostics

Objectives:
* Use the TTest procedure to analyze the differences between two population means
* Verify the assumptions of a two-sample test

**Assumptions**
- Independednt observations
- Normally distributed data for each group
- Equal variances for each group
- (Can use large smaples instead of normally distributed observations due to central limit theorem)



#### Folded F Test for Equality of Variances

```
H_o : sigma1^2 = sigma2^2
H_a : sigma1^2 ~= sigma2^2

F = max(s1^2, s2^2) / min(s1^2, s2^2)
```
The F value is calculated as a ratio of the greater of the two variances divided by the lesser of the two. F tends to be closer to 1.0 if the null hypothesis is true.
Test is valid **only** for independent samples from normal distributions. **Normality is even required for large samples!**

Note: If data is not normally distributed, you can look at plots to determine if equal variance.

![alt text](Pictures/Folded_F_Test.png "Folded F Test for Equality of Variances")

####TTEST Procedure

```
proc ttest data=_SAS-data-set_
	class _variable_;
	var _variables_;
	Paired _variable1*variable2_;
run;
```

Paired - specifies pairs of numeric response variables from which difference scores are calculated (variable1-variable2). A one-sample t test is then performed on the difference scores

####Steps for t-Test for equal/unequal means

1. Check the assumption of equal variances and then use the appropriate test for equal means
2. If calculated F > F' then use the equal variance t-test line in the output to test whether the means of the two populations are equal **(Pooled)**. Otherwise, use the unequal variance t-test **(Scatterthwaite)**.

**Before doing the appropriate test for equal means, you must do a test to see if the variances are equal.** 

####Demo: Two-sample t-test

```
/*st102d01.sas*/
proc ttest data=sasuser.TestScores plots(shownull)=interval;
    class Gender;
    var SATScore;
    title "Two-Sample t-test Comparing Girls to Boys";
run;
```

First you must verify assumptions of t-tests. Look at distribution of each gender to verify the normality of each group (looks fairly normal on page 2-9). 
Q-Q plots also approximate to a normal distritibution, with one outlier - a male scoring 1600 when no other male scored greater than 1400.


Result:
```

                                                        The TTEST Procedure
 
                                                        Variable:  SATScore

                           Gender          N        Mean     Std Dev     Std Err     Minimum     Maximum

                           Female         40      1221.0       157.4     24.8864       910.0      1590.0
                           Male           40      1160.3       130.9     20.7008       890.0      1600.0
                           Diff (1-2)            60.7500       144.8     32.3706                        

                   Gender        Method               Mean       95% CL Mean        Std Dev      95% CL Std Dev

                   Female                           1221.0      1170.7   1271.3       157.4       128.9    202.1
                   Male                             1160.3      1118.4   1202.1       130.9       107.2    168.1
                   Diff (1-2)    Pooled            60.7500     -3.6950    125.2       144.8       125.2    171.7
                   Diff (1-2)    Satterthwaite     60.7500     -3.7286    125.2                                 

                                    Method           Variances        DF    t Value    Pr > |t|

                                    Pooled           Equal            78       1.88      0.0643
                                    Satterthwaite    Unequal      75.497       1.88      0.0644

                                                       Equality of Variances
 
                                         Method      Num DF    Den DF    F Value    Pr > F

                                         Folded F        39        39       1.45    0.2545
```
1. Examine descriptive statistics for each group and their differences
2. Look at the Equality of Variances table. F test has a p-value of 0.2545 > 0.05. Therefore, do NOT reject the null hypothesis of equal variances. We continue as if the variances are equal.
3. Look at the T-Test table for the hypothesis for equal means. Use the equal variance (pooled) t-test.
	* Do not reject null hypothesis that the group means are equal P-value = 0.0643 > 0.05 . 
	* There is no significant difference in average SAT score between boys and girls
	* Note: you can also use confidence intervals. (-3.6950, 125.2) includes 0 (95% confidence interval)


### 2.2 One-Way ANOVA

![alt text](Pictures/Overview_of_statistical_models.png)

**Predictor** = Categorical 
**Response** = Continuous    ->   Use **One-Way ANOVA**

Analysis of variance is a statistical technique used to compare the means of two or more groups of observations or treatments.

*Examples of one-way anova*: 
- Do accountants, on average, earn more than teachers?
- Do people treated with one of two new drugs have higher average T-cell counts than people in the control group?
- Do people spend different amounts of depending on which type of credit card they have?


When there are three or more levels for the grouping variable, you can run a series of t tests between all the pairs of levels. However, a more powerful approach is to analyze all the data simultaneously _(one-way analysis of variance)_. In this case, **the test statistic is the F ratio** rather than the Student's t value.


####Garlic Ranch Example

-Does the type of fertilizer used affect the average weight of garlic grown at the Montana Gourmet Garlic Ranch?

Variables in the data set:
* `Fertilizer`: Type of fertilizer used (1 through 4) where 3 are organic, and one is a chemical fertilizer (as control)
* `BulbWt`: Average garlic bulb weight (in pounds) in the bed
* `Cloves`: The average number of cloves on each bulb
* `BedID`: randomly assigned bed identification number

Printing the data in the sasuse.MGGarlic dataset and create descriptive statistics:

```
/*st102d02.sas*/  /*Part A*/
proc print data=sasuser.MGGarlic (obs=10);
   title 'Partial Listing of Garlic Data';
run;
```

Data shown below:
```
                                      The MEANS Procedure

                                  Analysis Variable : BulbWt

             N
           Obs     N            Mean         Std Dev         Minimum         Maximum
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
            32    32           0.219           0.029           0.152           0.278
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ


                                  Analysis Variable : BulbWt

                     N
     Fertilizer    Obs     N            Mean         Std Dev         Minimum         Maximum
   ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
              1      9     9           0.225           0.025           0.188           0.254

              2      8     8           0.209           0.026           0.159           0.241

              3     11    11           0.230           0.026           0.189           0.278

              4      4     4           0.196           0.041           0.152           0.248
   ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
```

Look at fertilizer groups separately:
```
/*st102d02.sas*/  /*Part B*/
proc means data=sasuser.MGGarlic printalltypes maxdec=3;
    var BulbWt;
    class Fertilizer;
    title 'Descriptive Statistics of Garlic Weight';
run;

/*st102d02.sas*/  /*Part C*/
proc sgplot data=sasuser.MGGarlic;
    vbox BulbWt / category=Fertilizer datalabel=BedID;
    format BedID 5.;
    title "Box and Whisker Plots of Garlic Weight";
run;

```

`PRINTALLTYPES` displays all requested combinations of class variables (all TYPE vaalues) in the printed or displayed output.

`Class *variables*` specifies the variables whose values define the subgroup combinations for the analysis. Do not need to sort the data by class variables.

`Category =` produces separate box plots for each level of the variable listed

In this demo, the design is not balanced. The groups are not equally sized (look at N Obs in the Means procedure).

![alt text](Pictures/Anova_Null_Alternative_Hypothesis.png)


####Partitioning Variability in ANOVA

In ANOVA, Total Variation (measured by the corrected total sum of squares) is partitioned into two components:

1. The Between Group Variation (Model Sum of Squares)
	* Variability explained by the independent variable and therefore represented by the between treatment sum of squares. Calculated as the weighted (by group size) sum of the squared differences between the mean for each group and the overall mean. (SS_M) ![alt text](Pictures/Between_Group_Variation.png)
2. The Within Group variation (Error Sum of Squares)
  * Variability not explained by the model. Also referred to as _within treatement variability or residual sum of squares_. It is calculated as the sum of the squared differences between each observed value and the mean for its group. (SS_E) ![alt text](Pictures/Within_Group_Variation.png)

ANOVA breaks apart the variance of the dependent variable to determine whether the between-group variation is a significant portion of the total variation.

The test statistic (F Ratio) is only a ratio of the model variance to the error variance.

Overall variability in the response variable is calculated as the sum of the squared differences between each observed value and the overall mean. (SS_T) ![alt text](Pictures/Total_Variation.png)

**SS_T = SS_M + SS_E**

The amount of variability in the response that the model explains is given by: (SS_M / SS_T)*100%

The basic measures of variation under the two hypotheses are transformed into a ratio of the model and the error variances that has a known distribution (sample statistic, F ratio) under the null hypothesis that all group means are equal. The F ratio can be used to compute a p-value.

####F Statistic and Critical Values

F = MSM/MSE = SS_M/df_m  / SS_E/df_e

- **Model DF** = Number of treatments minus 1

- **Corrected total DF** = sample size - 1

- **Error DF** = the sample size minus the number of treatments (differenceb etween total DF and model DF)

Mean squares are calculated by taking the sums of squares and dividing by the corresponding degrees of freedom. They can be thought of as variances.
- MSE is an estimate of sigma^2, the constant variance assumed for all treatments
- If mu_i = mu_j for all i ~= j, then the mean square for the model (MSM) is also an estimate of sigma^2
- If mu_i ~= mu_j then MSM estimates sigma^2 plus a positive constant
- The p-value for the test is calculated from the F distribution with appropriate degrees of freedom 
	-(numerator = df_m, denominator = df_e)


R^2 = SS_M / SS_T

- Coefficient of determination is a measure of the proportion of variability explained


####The ANOVA Model

![alt text](Pictures/Fertilizer_Equation.png)
 
 Variables:
 - `Y_ik` = the kth value of the response variable for the ith treatment
 - `mu` = the overall population mean of the response
 - `tau_i` = the difference between the population mean of the ith treatment and the overall mean, mu. This is referred to as the *effect* of treatment i
 - `epsilon_ik` = the difference between the observed value of the kth observation in the ith group and the mean of the ith group (error term)


 #### Proc GLM

 Uses a parameterization of categorical variables in its `Class` statement that will not directly estimate the values of the parameters in the model shown. The correct parameter estimates can be obtained by adding the `solution` option in the `model` statment in `proc glm` and then using simple algebra.
 Parameter estimates and standard errors can also be obtained using the `estimate` statments.

 Fixed effect - researchers are only interested in four specific fertilizers
 Random effect - If the fertilizers used were a sample of many that can be used, the sampling variability of fertilizers would need to be taken into account in the model.

 ##### SAS code

 ```
/*st102d03.sas*/  /*Part A*/
proc glm data=sasuser.MGGarlic;
     class Fertilizer;
     model BulbWt=Fertilizer;
     title 'Testing for Equality of Means with PROC GLM';
run;
quit;


 ```

 * `Class` = specifies classification variables for analysis
 * `Model` = specifies dependent and independent variables for analysis
 * `Means` = computes unadjusted means of the dependent variable for each value of the speicified effect
 * `LSMeans` = produces adjusted means for the outcome variable, broken out by the variable specified and adjusting for any other exlanatory variables included in the `model` statment
 * `output` = specifies an output data set that contains all variables form the input data set and variables that represent statistics fromt he analysis

 ```

                                    Dependent Variable: BulbWt

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                        3      0.00457996      0.00152665       1.96    0.1432

      Error                       28      0.02183054      0.00077966

      Corrected Total             31      0.02641050


                      R-Square     Coeff Var      Root MSE    BulbWt Mean

                      0.173414      12.74520      0.027922       0.219082


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00457996      0.00152665       1.96    0.1432


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00457996      0.00152665       1.96    0.1432
```


Output divided into 3 parts
	- The analysis of variance table
	- Descriptive information
	- Information about the effect of the independent variable in the model


The F statistic and corresponding p-value are reported in the Analysis of Variance table. p-value (0.1432) > 0.05 and therefore, you do not reject the null hypothesis of no difference between the means.

The `BulbWt` Mean is the mean of all the data values in the variable `BulbWt` without regard to `Fertilizer`

For a one-way anova (only one classification variable) the informationa bout the independent variable in the model is an exact duplicate of the model line of the  anova table.

Default plot created is a box plot. *To check validity of Anova model, look at the diagnostic plots*. Code is below to do this:

 ```
/*st102d03.sas*/  /*Part B*/
proc glm data=sasuser.MGGarlic plots(only)=diagnostics;
     class Fertilizer;
     model BulbWt=Fertilizer;
     means Fertilizer / hovtest;
     title 'Testing for Equality of Means with PROC GLM';
run;
quit;

 ```

 Selected means statement option:

 `Hovtest` performs Levene's test for homogeneity (equality) of variances. The null hypothesis is that variances are equal (Levene's is default test)

 `Diagnostics` produces a panel display of diagnostic plots for linear models.

##### Diagnostic plots

Plot in upper left shows residuals vs fitted values from ANOVA model. Any patterns or trends in this plot indicates model misspecification

 Residual histogram and Q-Q plot at bottom left and middle left respectively test for normality assumption
 - Histogram has no unique peak and has short tales, but is approximately symmetric
 - Data values in Q-Q stay close to diagonal reference line. Strong support to assumption of normality distrbuted errors
 - You can look at the following table to check the assumptions of equal variances (output of hovtest option):

```
	             Levene's Test for Homogeneity of BulbWt Variance
                         ANOVA of Squared Deviations from Group Means

                                         Sum of        Mean
               Source            DF     Squares      Square    F Value    Pr > F

               Fertilizer         3    1.716E-6    5.719E-7       0.98    0.4173
               Error             28    0.000016    5.849E-7
```
Null hypothesis is that variances are equal over all the `fertilizer` groups. p-value 0.4173 > 0.05 and therefore you *do not* reject the jull hypothesis.

If variances were not equal, you could add the `welch` option to the `means` statement. 


####SUMMARY

Null Hypothesis: All means are equal
Alternative hypothesis: At least one mean is different

1. Produce descriptive statistics
2. Verify assumptions
	- Independence
	- Errors are normally distributed
	- Error variances are equal for all groups
3. Examine the p-value in the ANOVA table. If the p-value is less than alpha, reject the null hypothesis


### 2.3 ANOVA with Data from a Randomized Block Design

**Objectives**
- Recognize the difference between a completely randomized design aand a randomized block design
- differentiate between observed data and designed experiments
- Use the GLM procedure to analyze data from a randomized block design

**Observational or Retrospective Studies**
- Groups can be naturally occuring (gender ethnicity)
- Random assignment might be unethical or untenable (smoking or credit risk groups)
- Often you look at what already happened instead of following through to the future
- you have little control over other factors contributing to the outcome measure


Bulb weight = Fertilizer + Nuisance Factors + Random Variable

$\gamma_{ijk} = \mu + \alpha_i + \tau_j + \epsilon_{ijk}$

where Nuisance factors could be sun exposure, ph of the soil, rain

Before blocking, the variation due to the nuisance factors is contained within the sum of square errors

Because sector is included in the anova model, any effect caused by the nuisance factors that are common within a sector are accounted for in the Model Sum of Squares and not the Error Sum of Squares. Removing significant effects fromt he Error Sum of squares tends to give more power to the test of the effect of interest. This is because MSE, the denominator of the F statistic, tends to be reduced.

####Including a Blocking Variable in the Model

Additional assumptions are as follows:
- Treatments are randomly assigned within each block.
- The effects of the tratment factor are constant across the levels of the blocking variable

When the effects of the treatment factor are not constant across the levels of the other variable, then this condition is called **interaction**.

####Anova with Blocking in SAS

```
/*st102d04.sas*/
proc glm data=sasuser.MGGarlic_Block plots(only)=diagnostics;
     class Fertilizer Sector;
     model BulbWt=Fertilizer Sector;
     title 'ANOVA for Randomized Block Design';
run;
quit;
```

The blocking variable must be in the model and it must be listed in the class statement.

Output:
```
									Dependent Variable: BulbWt

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                       10      0.02307263      0.00230726       5.86    0.0003

      Error                       21      0.00826745      0.00039369

      Corrected Total             31      0.03134008


                      R-Square     Coeff Var      Root MSE    BulbWt Mean

                      0.736202      9.085064      0.019842       0.218398


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00508630      0.00169543       4.31    0.0162
      Sector                       7      0.01798632      0.00256947       6.53    0.0004


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      Fertilizer                   3      0.00508630      0.00169543       4.31    0.0162
      Sector                       7      0.01798632      0.00256947       6.53    0.0004
```


The overall test p-value = 0.0003 indicates that there are significant differences between the means of the garlic bulb weights across fertilizers or blocks (sectors). However, because there is more than one term in the model, you *cannot* tell whether the differences are due to differences among the fertilizers or differences across sectors.

Because our P-value for fertilizer is 0.0162, there is a difference between fertilizers
Because the p-value in our sector is 0.0004, there is a difference between each of the blocks, and therefore blocking was required.

Because we chose 4 specific fertilizers, we can only make inferences on those 4 specific fertilizers (fixed-effects model)

**Diagnostics:**
	- Good random scatter of residuals
	- Decent looking Q-Q test

Now we want to know which pairs of fertilizers are garlic bulb weights different form one another?


###2.4 ANOVA Post Hoc Tests

Objectives
* Perform pairwise comparisons among groups after finding a significant effect of an independent variable in ANOVA
* Demonstrate graphical features in proc GLM for performing post hoc tests
* Interpret a diffogram
* Interpret a control plot

When we control the **comparisonwise error rate** (CER), we fix the level of alpha for a single comparison, without taking into consideration all the pairwise comparisons we are making.

The **experimentwise error rate** (EER) uses an alpha that takes into consideration all the pariwise comparisons that you are making. Chance that you falsely conclude that at least one difference exists is much higher when you consider all possible comparisons.

Control Comparisonwise Error Rate -> Pairwise t-tests

Control Experimentwise Error Rate -> Compair all pairs tukey, compare to control dunnett

All of the multiple comparison methods are requested with options in the `LSMEANS` statment of `proc glm`

**Comparison Control** :  `lsmeans / pdiff=all adjust=t`

**Experimentwise Control**  :  `lsmeans / pdiff=all adjust=tukey` or 
							`pdiff=control('control level') adjust=dunnett`

Tukey's multiple comparison method:
* Appropriate when you consider pairwise comparisons only
* The experimentwise error rate is
	* Equal to alpha when all pairwise comparisons are considered
	* Less than alpha when fewer than all pairwise comparisons are considered

**Use a diffogram to tell whether two group means are statistically significant**

![alt text](Pictures/Diffogram.PNG)


#### Special case of comparing to a control
Comparing to a control is appropriate when there is a natural reference group (ie placebo in a drug trial)

This is an example of the dunnet method

![alt text](Pictures/Control_Plot.png)

If confidence interval is in shaded region, then they are not statistically different.

####Post Hoc Pairwise Comparisons in SAS

```
/*st102d05.sas*/
proc glm data=sasuser.MGGarlic_Block 
         plots(only)=(controlplot diffplot(center));
    class Fertilizer Sector;
    model BulbWt=Fertilizer Sector;
    lsmeans Fertilizer / pdiff=all adjust=tukey;
    lsmeans Fertilizer / pdiff=control('4') adjust=dunnett;
    lsmeans Fertilizer / pdiff=all adjust=t;
    title 'Garlic Data: Multiple Comparisons';
run;
quit;

```
`plot=` **options**:

`controlplot` = requests display in which least squares means are compared against a reference level. LS mean control plots only produced when you specify `pdiff=control` or `adjust=dunnett` in lsmeans statement

`diffplot` = modifies diffogram produced by lsmeans statement with the `pdiff=all` option. `center` marks center point for each comparison.

`lsmeans` **options**:

`pdiff=` = requests p-values for differences, which is the probability of seeing a difference between two means that is as large as the observed means or larger if the two populations means are actually the same.

`adjust=` specifies the adjustment method for multiple comparisons.
* `T` asks that no adjustment made for multiple comparisons
* `Tukey` uses Tukey
* `Dunnet` uses dunnet

Results below show p-values the diffogram represents:

```
                                      Least Squares Means

                                                BulbWt      LSMEAN
                            Fertilizer          LSMEAN      Number

                            1               0.23625000           1
                            2               0.21115125           2
                            3               0.22330125           3
                            4               0.20288875           4


                          Least Squares Means for effect Fertilizer
                             Pr > |t| for H0: LSMean(i)=LSMean(j)

                                  Dependent Variable: BulbWt

                 i/j              1             2             3             4

                    1                      0.0195        0.2059        0.0029
                    2        0.0195                      0.2342        0.4143
                    3        0.2059        0.2342                      0.0523
                    4        0.0029        0.4143        0.0523


NOTE: To ensure overall protection level, only probabilities associated with pre-planned
      comparisons should be used.
```

By Tukey test, 1 and 4 are different.
By Dunnett, 1 and 4 are different
By T test, 1 and 4 are different and 2 and 1 are different (NOT TRUE DONT USE THIS TEST)


### 2.5 Two-Way ANOVA with Interactions

Objectives
* Fit a two-way ANOVA model
* Detect interactions between factors
* Analyze the treatments when there is a significant interaction

N-way ANOVA

![alt text](Pictures/Nway_Anova.png)

Drug example: The purpose of the study is to look at the effect of a new prescription drug on blood pressure

`DrugDose` dosage level of drug (1, 2, 3, 4) corresponding to (Placebo, 50mg, 100mg, 200mg)

`Disease` heart disease category

`BloodP` change in diastolic blood pressure after 2 weeks treatment

![alt text](Pictures/BloodP_Model.png)

An interaction occurs when the differences between group means on one variable change at different levels of another variable.

If an interaction exists between any factors, the tests for the individual factor effects might be misleading, deo to masking of the effects by the interaction. This is especially true for unbalanced data.

If the lines of an interaction graph are parallel, we can say that there is no interaction between variables.

Analyze the main effects with the interaction in the model:

$\Gamma_{ijk} = \mu + \alpha_i + \Beta_j + (\alpha\beta)_{ij} + \epsilon_{ijk}$

or delete the interaction from the model and then analyze the main effects:

$\Gamma_{ijk} = \mu + \alpha_i + \Beta_j + \epsilon_{ijk}$

#### Two-Way ANOVA with Interactions in SAS

```
/*st102d06.sas*/  /*Part A*/
proc print data=sasuser.drug(obs=10);
    title 'Partial Listing of Drug Data Set';
run;

/*st102d06.sas*/  /*Part B*/
proc format;
    value dosefmt 1='Placebo'
                  2='50 mg'
                  3='100 mg'
                  4='200 mg';
run;

proc means data=sasuser.drug
           mean var std nway;
    class Disease DrugDose;
    var BloodP;
  format DrugDose dosefmt.;
    output out=means mean=BloodP_Mean;
    title 'Selected Descriptive Statistics for Drug Data Set';
run;
```

`nway` = when you include `class` variables, `nway` specifies that the output data set contains only statistics for the observations with the hightest type and way values.

Results:

```

                                  Analysis Variable : BloodP

                      Drug         N
           Disease    Dose       Obs            Mean        Variance         Std Dev
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
           A          Placebo     12       1.3333333     183.1515152      13.5333483

                      50 mg       16      -9.6875000     356.7625000      18.8881577

                      100 mg      13     -26.2307692     329.0256410      18.1390640

                      200 mg      18     -22.5555556     445.0849673      21.0970369

           B          Placebo     15      -8.1333333     285.9809524      16.9109714

                      50 mg       15       5.4000000     479.1142857      21.8886794

                      100 mg      14      24.7857143     563.7197802      23.7427838

                      200 mg      13      23.2307692     556.3589744      23.5872630

           C          Placebo     14       0.4285714     411.8021978      20.2929100

                      50 mg       13      -4.8461538     577.6410256      24.0341637

                      100 mg      14      -5.1428571     195.5164835      13.9827209

                      200 mg      13       1.3076923     828.5641026      28.7847894
           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ

```



Let's plot it!

```
/*st102d06.sas*/  /*Part C*/
proc sgplot data=means;
    series x=DrugDose y=BloodP_Mean / group=Disease markers;
    xaxis integer;
    title 'Plot of Stratified Means in Drug Data Set';
  format DrugDose dosefmt.;
run;
```

`series` statement creates a line plot

`markers` adds data point markers to the series plot data points

`xaxis integer` forces the x-axis to have tick marks only at integer values


```
/*st102d06.sas*/  /*Part D*/
proc glm data=sasuser.drug order=internal;
    class DrugDose Disease;
    model Bloodp=DrugDose Disease DrugDose*Disease;
    title 'Analyze the Effects of DrugDose and Disease';
    title2 'Including Interaction';
  format DrugDose dosefmt.;
run;
quit;
```

results:

```

Dependent Variable: BloodP

                                              Sum of
      Source                      DF         Squares     Mean Square    F Value    Pr > F

      Model                       11      36476.8353       3316.0759       7.66    <.0001

      Error                      158      68366.4589        432.6991

      Corrected Total            169     104843.2941


                      R-Square     Coeff Var      Root MSE    BloodP Mean

                      0.347918     -906.7286      20.80142      -2.294118


      Source                      DF       Type I SS     Mean Square    F Value    Pr > F

      DrugDose                     3        54.03137        18.01046       0.04    0.9886
      Disease                      2     19276.48690      9638.24345      22.27    <.0001
      DrugDose*Disease             6     17146.31698      2857.71950       6.60    <.0001


      Source                      DF     Type III SS     Mean Square    F Value    Pr > F

      DrugDose                     3       335.73526       111.91175       0.26    0.8551
      Disease                      2     18742.62386      9371.31193      21.66    <.0001
      DrugDose*Disease             6     17146.31698      2857.71950       6.60    <.0001

```

Although the P-value for drug dose is high (almost 1), it does not mean that there is no effect in drug dose. If there is an interaction between drug dose and disease, we have to keep our main effect.

Looking on the graph, you can see that the drug dose relationship for disease A and disease B 'cancel each other out'



```
/*st102d06.sas*/  /*Part E*/
ods graphics off;
ods select LSMeans SlicedANOVA;
proc glm data=sasuser.drug order=internal;
    class DrugDose Disease;
    model Bloodp=DrugDose Disease DrugDose*Disease;
    lsmeans DrugDose*Disease / slice=Disease;
    title 'Analyze the Effects of DrugDose';
    title2 'at Each Level of Disease';
  format DrugDose dosefmt.;
run;
quit;

ods graphics on;

```

Results for lsmeans:

```
 DrugDose*Disease Effect Sliced by Disease for BloodP

                                       Sum of
            Disease        DF         Squares     Mean Square    F Value    Pr > F

            A               3     6320.126747     2106.708916       4.87    0.0029
            B               3           10561     3520.222833       8.14    <.0001
            C               3      468.099308      156.033103       0.36    0.7815

```

Taking disease x, puts it in its own data set, and run an anova against drug dose




# Day 5

## Chapter 5: Categorical Data Analysis

### 5.1 Describing Categorical Data

Objectives
* Examine the distribution of categorical variables
* Do preliminary examinations of associans between variables

Categorical variables association
* An associacion exists between two categorical variables if the distribution of one variable changes when the other variable changes
* For no assiciation, the distribtion of the first variable stays constant for different levels of the other variable

#### Crosstabulation Tables

Crosstabulation table shows the number of observations for each combination of the row and column variables.

By default, 4 measures in each cell
- Frequency
- Percent
- Row Percent
- Column Percent

#### Proc Freq

```
proc freq data=sas-data-set;
  tables table-requests<l options>;
run;
```

`tables` requests tables and specifies options for producing tests.
- General form is variable1*variable2*... where any number can be put in the tables statment.
  - For 2 way, first represents rows and second represents columns

#### Titanic example

- 2,223 passengers
- 1,517 fatalities

Variables:
- Survival statuss (1 or 0)
- Age
- Gender
- Class (1,2,3)
- Fare (cumulative total for a purchase for each person in a party)

Code:

```
/*st105d01.sas*/
title;
proc format;
    value survfmt 1 = "Survived"
                  0 = "Died"
                  ;
run;

proc freq data=sasuser.Titanic;
    tables Survived Gender Class
           Gender*Survived Class*Survived /
           plots(only)=freqplot(scale=percent);
    format Survived survfmt.;
run;

proc univariate data=sasuser.Titanic noprint;
    class Survived;
    var Age ;
    histogram Age;
    inset mean std median min max / format=5.2 position=ne;
    format Survived survfmt.;
run;
```

`FREQPLOT(<suboptions>)` requests a frequency plot. Available for frequency and crosstabulation tables

`(Scale=)` specifies the scale of the frequencies to display. Default is `Scale=freq`. Can also use `scale=percent`

In proc univariate, the line that reads:
`inset mean std median min max / format=5.2 position=ne;`
will put all of those variables in the same plot of the histogram.

We look at the distribution of survived, gender and class. Also look at the crosstabulation of gender*survived, and class*survived.

```

                                      The FREQ Procedure

                                                      Cumulative    Cumulative
                 Survived    Frequency     Percent     Frequency      Percent
                 ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                 Died             809       61.80           809        61.80
                 Survived         500       38.20          1309       100.00


                                                     Cumulative    Cumulative
                  Gender    Frequency     Percent     Frequency      Percent
                  ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                  female         466       35.60           466        35.60
                  male           843       64.40          1309       100.00


                                                    Cumulative    Cumulative
                  Class    Frequency     Percent     Frequency      Percent
                  ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                      1         323       24.68           323        24.68
                      2         277       21.16           600        45.84
                      3         709       54.16          1309       100.00


                                  Table of Gender by Survived

                              Gender     Survived

                              Frequency‚
                              Percent  ‚
                              Row Pct  ‚
                              Col Pct  ‚Died    ‚Survived‚  Total
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              female   ‚    127 ‚    339 ‚    466
                                       ‚   9.70 ‚  25.90 ‚  35.60
                                       ‚  27.25 ‚  72.75 ‚
                                       ‚  15.70 ‚  67.80 ‚
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              male     ‚    682 ‚    161 ‚    843
                                       ‚  52.10 ‚  12.30 ‚  64.40
                                       ‚  80.90 ‚  19.10 ‚
                                       ‚  84.30 ‚  32.20 ‚
                              ƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                              Total         809      500     1309
                                          61.80    38.20   100.00


```

More women survived while more men died according to our crosstabulation table. It also turns out that as the higher class you are, the more chance you have of surviving (most of the lower class were men).

### 5.2 Tests of Association

Objectives
- Perform a chi-square test for association
- Examine the strength of association
- Calculate exact p-values
- Perform a Mantel-Haenszel chi-square test

Using contingency table analysis!

Assumng the same titanic example:

**Null Hypothesis** = There is no association between gender and surival. The probability of surviving the titanic class was the same whether you were male or female

**Alternative Hypothesis** = There **is** an association between gender and survival.


#### Chi-Square Test

**No Association** - Observed frequencies = Expected frequencies

**Association** - Observed frequencies ~= Expected frequencies

![alt text](Pictures/chi_squared_eq.png)

This equation tests whether an association exists. It **does not** measure the strength of an association. It depends on sample size.

The p-value for the chi-square test only indicates how confident you can be that the null hypothesis of no association is false. Doubling the size of the sample by duplicating each observation will double the value of the chi-square statistic, even though the strength of the association does not change.

**Cramer's V statistic** can measure the strength of the association.

- Has a range of -1 to 1 for 2 by 2 tables and 0 to 1 for larger tables. Values farther from 0 indicate stronger association

#### Odds Ratio

Odds ratio indicates how much more likely, **with respect to odds**, a certain event occurs in one group relative to its occurence in another group

![alt test](Pictures/odds_eq.png)

**Odds** are calculated **from** probabilities.

**Odds ratio** is the ratio of odds, as seen below:

![alt test](Pictures/odds_ratio_ex.png)

Group A had 1/3 the odds of group B **OR** Group B had 3 time the odds of having the outcome compared to group A.

#### Chi-Square Test in SAS

```
/*st105d02.sas*/
ods graphics off;
proc freq data=sasuser.Titanic;
    tables (Gender Class)*Survived
          / chisq expected cellchi2 nocol nopercent 
            relrisk;
    format Survived survfmt.;
    title1 'Associations with Survival';
run;

ods graphics on;
```

`Chisq` produces chi-square test of association and the measures of association based on chi-square statistic

`Expected` prints the expected cell frequencies under the hypothesis of no association

`CellChi2` prints each cell's contribution to th total chi-square statistic

`Nocol` suppresses column percentages

`Nopercent`suppresses cell percentages

`Relrisk` prints a table with risk ratios (probability ratios) and odds ratios

Results:

```
                       The FREQ Procedure

                                  Table of Gender by Survived

                           Gender          Survived

                           Frequency      ‚
                           Expected       ‚
                           Cell Chi-Square‚
                           Row Pct        ‚Died    ‚Survived‚  Total
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           female         ‚    127 ‚    339 ‚    466
                                          ‚    288 ‚    178 ‚
                                          ‚ 90.005 ‚ 145.63 ‚
                                          ‚  27.25 ‚  72.75 ‚
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           male           ‚    682 ‚    161 ‚    843
                                          ‚    521 ‚    322 ‚
                                          ‚ 49.753 ‚ 80.501 ‚
                                          ‚  80.90 ‚  19.10 ‚
                           ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆƒƒƒƒƒƒƒƒˆ
                           Total               809      500     1309


                          Statistics for Table of Gender by Survived

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     1    365.8869    <.0001
                    Likelihood Ratio Chi-Square    1    372.9213    <.0001
                    Continuity Adj. Chi-Square     1    363.6179    <.0001
                    Mantel-Haenszel Chi-Square     1    365.6074    <.0001
                    Phi Coefficient                      -0.5287
                    Contingency Coefficient               0.4674
                    Cramer's V                           -0.5287


                                     Fisher's Exact Test
                              ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                              Cell (1,1) Frequency (F)       127
                              Left-sided Pr <= F       7.351E-83
                              Right-sided Pr >= F         1.0000

                              Table Probability (P)    6.705E-83
                              Two-sided Pr <= P        7.918E-83
```

The cell for survived=1 and Gender=female has the highest Chi-Square value (145.63). It contributes the most to the chi-square statistic.

p-value for chi-square statistic is <.0001 < 0.05. We reject null hypothesis. **There is evidence of an association between gender and survived**.

Strength of this association is realatively strong since Cramer's V is -0.5287

```
 The FREQ Procedure

                          Statistics for Table of Gender by Survived

                          Estimates of the Relative Risk (Row1/Row2)

               Type of Study                   Value       95% Confidence Limits
               ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
               Case-Control (Odds Ratio)      0.0884        0.0677        0.1155
               Cohort (Col1 Risk)             0.3369        0.2894        0.3921
               Cohort (Col2 Risk)             3.8090        3.2797        4.4239

                                      Sample Size = 1309

```

**Relative Risk** table shows another measure of strength association.
- Odds ratio shown in first row of table with 95% confidence limits. Interpreted as the odds of a top row value to be in the left column compared with the same odds in the bottom row.
- In this case, a female has 0.0884 (9%) of the odds of dying compared with a male.

**Cohort estimates** for each column interpreted as **probability ratios, not odds ratios**. You get the choice of assessing probabilities of left column (col1) or right column (col2).
  - The col1 risk shows the ratio of probabilities of females to males being in the left column (27.25/80.90)=0.3369

##### When not to use Chi-Squared

Do not use assymptotic chi-squared test when more than 20% of cells have expected counts less than 5!

p-values are based on the assumption that the test statistic follows a particular distribution when the sample size is sufficiently large.

For small sample size, we calculate **Exact** p-values

For large sample size, we calculate **Asymptotic** p-values. It takes a lot of time and memory to compute exact p-values since SAS calculates *every* possible combination of crosstabulation frequencies.

Exact p-values for Pearson Chi-Square

![alt text](Pictures/exact_pvalues_pearsonchi.png)

The exact p-value is the sum of probabilities of all tables with X^2 values **as great or greater** than that of the observed table!

#### Exact p-values for Pearson Chi-Square in SAS

```
/*st105d03.sas*/
ods graphics off;
proc freq data=sasuser.exact;
   tables A*B / chisq expected cellchi2 nocol nopercent;
   title "Exact P-Values";
run;

ods graphics on;

```

Results:

```
                               Statistics for Table of A by B

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     1      2.1000    0.1473
                    Likelihood Ratio Chi-Square    1      2.8306    0.0925
                    Continuity Adj. Chi-Square     1      0.3646    0.5460
                    Mantel-Haenszel Chi-Square     1      1.8000    0.1797
                    Phi Coefficient                      -0.5477
                    Contingency Coefficient               0.4804
                    Cramer's V                           -0.5477

                     WARNING: 100% of the cells have expected counts less
                              than 5. Chi-Square may not be a valid test.


                                     Fisher's Exact Test
                              ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                              Cell (1,1) Frequency (F)         0
                              Left-sided Pr <= F          0.2857
                              Right-sided Pr >= F         1.0000

                              Table Probability (P)       0.2857
                              Two-sided Pr <= P           0.4286

                                        Sample Size = 7

```

Warning suggests that p-value should not be trusted.

Want to report the two-sided Pr <= P value.

Difference between exact p-value (0.4286) and asymptotic p-value (0.1473). Exact tests tend to be more conservative than asymptotic tests


#### Ordinal Variables - Mantel-Haenszel Chi-square test

**Null Hypothesis** : There is no ordinal association between the row and column variables

**Alternative Hypothesis** : There is an ordinal association between the row and column variables

The Mantel-Haenszel Chi-Square Test determines whether an ordinal association exists. **Does Not** measure the strength of the ordinal association
- More powerful than the general association chi-squared statistic for detecting an ordinal association
  - All of the Mantel-Haenszel statistic's power is concentrated toward that objective
  - Power of the general association statistic is dispersed over a greater number of alternatives

#### Spearman Correlation Statistic

To measure strength of ordinal association, you can use Spearman
- Has range between -1 and 1
- Has values close to 1 if there is relatively high degree of positive correlation
- Has values close to -1 if there is a relatively high degree of negative correlation
- Appropriate only if both variables are ordinal scaled and the values are in logical order

**Spearman** correlation uses ranks of the data

**Pearson** correlation uses the observed values when variable is numeric

#### Using SAS

```
/*st105d04.sas*/
ods graphics off;
proc freq data=sasuser.Titanic;
    tables Class*Survived / chisq measures cl;
    format Survived survfmt.;
    title1 'Ordinal Association between CLASS and SURVIVAL?';
run;

ods graphics on;
```

`chisq` produces Pearson chi-square, likelihood-ratio chi-square, and the Mantel-Haenszel chi-square. Also produces measures of association based on chi-square such as the phi coefficient, contingency coefficient and Cramer's V

`measures` produces Spearman correlation statistic and other measures of association

`CL` produces confidence bounds for the `measures` statistics

Results of mantel-Haenszel:

```
                         Statistics for Table of Class by Survived

                    Statistic                     DF       Value      Prob
                    ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
                    Chi-Square                     2    127.8592    <.0001
                    Likelihood Ratio Chi-Square    2    127.7655    <.0001
                    Mantel-Haenszel Chi-Square     1    127.7093    <.0001
                    Phi Coefficient                       0.3125
                    Contingency Coefficient               0.2983
                    Cramer's V                            0.3125

```

Results of spearman:

```

                           Statistics for Table of Class by Survived

                                                                          95%
         Statistic                              Value       ASE     Confidence Limits
         ƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒƒ
         Gamma                                -0.5067    0.0375    -0.5801    -0.4332
         Kendall's Tau-b                      -0.2948    0.0253    -0.3444    -0.2451
         Stuart's Tau-c                       -0.3141    0.0274    -0.3677    -0.2604

         Somers' D C|R                        -0.2613    0.0226    -0.3056    -0.2170
         Somers' D R|C                        -0.3326    0.0286    -0.3887    -0.2765

         Pearson Correlation                  -0.3125    0.0267    -0.3647    -0.2602
         Spearman Correlation                 -0.3097    0.0266    -0.3619    -0.2576

         Lambda Asymmetric C|R                 0.1540    0.0331     0.0892     0.2188
         Lambda Asymmetric R|C                 0.0317    0.0320     0.0000     0.0944
         Lambda Symmetric                      0.0873    0.0292     0.0301     0.1445

         Uncertainty Coefficient C|R           0.0734    0.0127     0.0485     0.0983
         Uncertainty Coefficient R|C           0.0485    0.0084     0.0320     0.0650
         Uncertainty Coefficient Symmetric     0.0584    0.0101     0.0386     0.0782

                                      Sample Size = 1309

```

Spearman correlation (-0.3097) indicates that there is amoderate, negative ordinal relationship between class and survival

ASE is asymptotic standard error (0.0266). Measure of standard error for larger samples (larger than 25 foe each degree of freedom in the Pearson chi-square stat)

95% confidence interval (-0.3619, -0.2576) for spearman correlation statistic does not conatain 0, and tehrefore the relationship is significant at the 0.05 significance level.


### Introduction to Logistic Regression

Objectives
- Define the concepts of logistic regression
- Fit a binary logistic regression model using the `logistic` procedure
- Describe the standard output form the logistic procedure with one continuous predictor variable
- read and interpret odds ratio talbes and plots

![alt text](Pictures/cont_vs_cat.png)

If response variable is dichotomous (two categories), appropriate logistic regression model is binary logistic regression

If more than 2 categories within response variable, then there are 2 possibilities
- If response is nominal -> fit a *nominal* logistic regression model
- If response is ordinal -> fit a *ordinal* logistic regression model

![alt text](Pictures/ols_reg.png)     vs    ![alt text](Pictures/lpm_model.png)

But can we really use a linear mondel?

Also, there is no such thing as an observed probability -> least squares methods cannot be used since there are no residuals.

Logistic regression model:

![alt text](Pictures/log_reg_model.png)

Logit Transformation:

![alt text](Pictures/logit_t.png)

`i` = indexes all cases (observations)

`p_i` = probability that the even (ex. a sale) occurs in the ith case

`ln` = natural log 

- Modeling probability has bounds of 0 and 1
- The logit has no upper or lower bounds
- Logit is the natural log of the odds

##### Assumption in logistic regression

Logit has a linear relationship with the predictor variables.

We are assuming that the relationship between our variable and probability follows the 'S' curve. This is so that we get a linear relationship between the logit and our variable.

Our logistic regression model therefore becomes:

![alt text](real_log_reg_model.PNG)

